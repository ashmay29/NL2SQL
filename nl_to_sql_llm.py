# -*- coding: utf-8 -*-
"""NL-to-SQL-LLM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fncAr2dD6sPDAhkCR1vl3xV1prLvs8Im
"""

# Install required packages
!pip install -q google-generativeai
!pip install -q sentence-transformers
!pip install -q faiss-cpu
!pip install -q networkx
!pip install -q matplotlib
!pip install -q sqlparse
!pip install -q pandas
!pip install -q numpy

import os
import json
import sqlite3
import re
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass, field
from datetime import datetime
import hashlib

import google.generativeai as genai
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
import pandas as pd
import sqlparse
import networkx as nx
import matplotlib.pyplot as plt
from collections import defaultdict
import warnings
warnings.filterwarnings('ignore')

class Config:
    GEMINI_API_KEY = "AIzaSyASLSu_D6VD539YgCpmayjxElyhVNgOObk"
    GEMINI_MODEL = "gemini-2.5-flash"
    EMBEDDING_MODEL = "all-MiniLM-L6-v2"

    CACHE_SIMILARITY_THRESHOLD = 0.85
    MAX_CACHE_SIZE = 1000

    MAX_CONTEXT_TURNS = 5
    COMPLEXITY_THRESHOLD = 3

    MAX_CORRECTION_ATTEMPTS = 2

genai.configure(api_key=Config.GEMINI_API_KEY)
model = genai.GenerativeModel(Config.GEMINI_MODEL)

print("Configuration loaded!")
print(f"Using model: {Config.GEMINI_MODEL}")

class DatabaseSetup:
    @staticmethod
    def create_sample_database(db_path: str = "ecommerce.db"):
        """Create a comprehensive sample database"""
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()

        cursor.execute("DROP TABLE IF EXISTS order_items")
        cursor.execute("DROP TABLE IF EXISTS orders")
        cursor.execute("DROP TABLE IF EXISTS products")
        cursor.execute("DROP TABLE IF EXISTS customers")
        cursor.execute("DROP TABLE IF EXISTS categories")

        cursor.execute("""
        CREATE TABLE customers (
            customer_id INTEGER PRIMARY KEY,
            name TEXT NOT NULL,
            email TEXT UNIQUE,
            join_date TEXT,
            country TEXT,
            total_spent REAL DEFAULT 0
        )
        """)

        cursor.execute("""
        CREATE TABLE categories (
            category_id INTEGER PRIMARY KEY,
            category_name TEXT NOT NULL,
            description TEXT
        )
        """)

        cursor.execute("""
        CREATE TABLE products (
            product_id INTEGER PRIMARY KEY,
            product_name TEXT NOT NULL,
            category_id INTEGER,
            price REAL NOT NULL,
            stock_quantity INTEGER DEFAULT 0,
            FOREIGN KEY (category_id) REFERENCES categories(category_id)
        )
        """)

        cursor.execute("""
        CREATE TABLE orders (
            order_id INTEGER PRIMARY KEY,
            customer_id INTEGER,
            order_date TEXT,
            total_amount REAL,
            status TEXT,
            FOREIGN KEY (customer_id) REFERENCES customers(customer_id)
        )
        """)

        cursor.execute("""
        CREATE TABLE order_items (
            order_item_id INTEGER PRIMARY KEY,
            order_id INTEGER,
            product_id INTEGER,
            quantity INTEGER,
            price_per_unit REAL,
            FOREIGN KEY (order_id) REFERENCES orders(order_id),
            FOREIGN KEY (product_id) REFERENCES products(product_id)
        )
        """)

        customers_data = [
            (1, 'John Doe', 'john@example.com', '2023-01-15', 'USA', 1500.00),
            (2, 'Jane Smith', 'jane@example.com', '2023-02-20', 'UK', 2300.50),
            (3, 'Bob Johnson', 'bob@example.com', '2023-03-10', 'Canada', 890.00),
            (4, 'Alice Brown', 'alice@example.com', '2023-04-05', 'USA', 3200.00),
            (5, 'Charlie Wilson', 'charlie@example.com', '2023-05-12', 'Australia', 1100.00),
        ]
        cursor.executemany("INSERT INTO customers VALUES (?, ?, ?, ?, ?, ?)", customers_data)

        categories_data = [
            (1, 'Electronics', 'Electronic devices and gadgets'),
            (2, 'Clothing', 'Apparel and fashion items'),
            (3, 'Books', 'Physical and digital books'),
            (4, 'Home & Garden', 'Home improvement and garden supplies'),
        ]
        cursor.executemany("INSERT INTO categories VALUES (?, ?, ?)", categories_data)

        products_data = [
            (1, 'Laptop Pro', 1, 1299.99, 50),
            (2, 'Wireless Mouse', 1, 29.99, 200),
            (3, 'USB-C Cable', 1, 12.99, 500),
            (4, 'T-Shirt', 2, 19.99, 150),
            (5, 'Jeans', 2, 49.99, 100),
            (6, 'Python Programming Book', 3, 39.99, 75),
            (7, 'Garden Tools Set', 4, 89.99, 30),
            (8, 'Smart Watch', 1, 299.99, 80),
        ]
        cursor.executemany("INSERT INTO products VALUES (?, ?, ?, ?, ?)", products_data)

        orders_data = [
            (1, 1, '2024-01-10', 1329.98, 'completed'),
            (2, 2, '2024-01-15', 1599.97, 'completed'),
            (3, 1, '2024-02-01', 169.98, 'completed'),
            (4, 3, '2024-02-10', 89.99, 'shipped'),
            (5, 4, '2024-02-15', 329.98, 'completed'),
            (6, 2, '2024-03-01', 699.98, 'processing'),
            (7, 5, '2024-03-05', 1299.99, 'completed'),
        ]
        cursor.executemany("INSERT INTO orders VALUES (?, ?, ?, ?, ?)", orders_data)

        order_items_data = [
            (1, 1, 1, 1, 1299.99),
            (2, 1, 2, 1, 29.99),
            (3, 2, 1, 1, 1299.99),
            (4, 2, 8, 1, 299.99),
            (5, 3, 4, 5, 19.99),
            (6, 3, 5, 1, 49.99),
            (7, 4, 7, 1, 89.99),
            (8, 5, 8, 1, 299.99),
            (9, 5, 2, 1, 29.99),
            (10, 6, 8, 2, 299.99),
            (11, 7, 1, 1, 1299.99),
        ]
        cursor.executemany("INSERT INTO order_items VALUES (?, ?, ?, ?, ?)", order_items_data)

        conn.commit()
        conn.close()

        print(f"Sample database created: {db_path}")
        return db_path

db_path = DatabaseSetup.create_sample_database()

class SchemaVisualizer:
    """Visualizes database schema as a graph showing tables and relationships"""

    def __init__(self, db_path: str):
        self.db_path = db_path
        self.schema_info = self._extract_schema()

    def _extract_schema(self) -> Dict[str, Any]:
        """Extract complete schema information"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        schema = {
            'tables': {},
            'relationships': []
        }

        cursor.execute("SELECT name FROM sqlite_master WHERE type='table'")
        tables = [row[0] for row in cursor.fetchall()]

        for table in tables:
            cursor.execute(f"PRAGMA table_info({table})")
            columns = cursor.fetchall()


            cursor.execute(f"PRAGMA foreign_key_list({table})")
            foreign_keys = cursor.fetchall()

            schema['tables'][table] = {
                'columns': [
                    {
                        'name': col[1],
                        'type': col[2],
                        'not_null': bool(col[3]),
                        'primary_key': bool(col[5])
                    }
                    for col in columns
                ],
                'foreign_keys': foreign_keys
            }

            for fk in foreign_keys:
                schema['relationships'].append({
                    'from_table': table,
                    'from_column': fk[3],
                    'to_table': fk[2],
                    'to_column': fk[4]
                })

        conn.close()
        return schema

    def visualize_schema(self, figsize=(14, 10)):
        """Create a graph visualization of the database schema"""
        G = nx.DiGraph()

        for table_name, table_info in self.schema_info['tables'].items():
            columns_str = "\\n".join([
                f"{'key' if col['primary_key'] else '  '} {col['name']}: {col['type']}"
                for col in table_info['columns'][:5]
            ])
            if len(table_info['columns']) > 5:
                columns_str += f"\\n  ... ({len(table_info['columns']) - 5} more)"

            label = f"{table_name}\\n{'-' * 20}\\n{columns_str}"
            G.add_node(table_name, label=label)

        edge_labels = {}
        for rel in self.schema_info['relationships']:
            G.add_edge(rel['from_table'], rel['to_table'])
            edge_labels[(rel['from_table'], rel['to_table'])] = (
                f"{rel['from_column']} â†’ {rel['to_column']}"
            )

        plt.figure(figsize=figsize)
        pos = nx.spring_layout(G, k=2, iterations=50)

        nx.draw_networkx_nodes(G, pos, node_color='lightblue',
                               node_size=8000, node_shape='s', alpha=0.9)

        nx.draw_networkx_edges(G, pos, edge_color='gray',
                               arrows=True, arrowsize=20,
                               arrowstyle='->', width=2)

        labels = nx.get_node_attributes(G, 'label')
        nx.draw_networkx_labels(G, pos, labels, font_size=8,
                                font_family='monospace')

        nx.draw_networkx_edge_labels(G, pos, edge_labels,
                                     font_size=7, font_color='red')

        plt.title("Database Schema Graph", fontsize=16, fontweight='bold')
        plt.axis('off')
        plt.tight_layout()
        plt.show()

        print("\n Schema Summary:")
        print(f"   Tables: {len(self.schema_info['tables'])}")
        print(f"   Relationships: {len(self.schema_info['relationships'])}")

    def get_schema_text(self) -> str:
        """Get schema as formatted text for LLM"""
        text = "DATABASE SCHEMA:\n\n"

        for table_name, table_info in self.schema_info['tables'].items():
            text += f"Table: {table_name}\n"
            text += "Columns:\n"
            for col in table_info['columns']:
                pk = " (PRIMARY KEY)" if col['primary_key'] else ""
                nn = " NOT NULL" if col['not_null'] else ""
                text += f"  - {col['name']}: {col['type']}{pk}{nn}\n"

            if table_info['foreign_keys']:
                text += "Foreign Keys:\n"
                for fk in table_info['foreign_keys']:
                    text += f"  - {fk[3]} â†’ {fk[2]}.{fk[4]}\n"
            text += "\n"

        return text

visualizer = SchemaVisualizer(db_path)
visualizer.visualize_schema()
print("\n" + visualizer.get_schema_text())

class SchemaLinker:
    """Maps user terms to database schema elements using embeddings and LLM"""
    def __init__(self, schema_info: Dict[str, Any]):
        self.schema_info = schema_info
        self.embedding_model = SentenceTransformer(Config.EMBEDDING_MODEL)
        self._build_schema_index()
    def _build_schema_index(self):
        """Build searchable index of schema elements"""
        self.schema_elements = []

        for table_name, table_info in self.schema_info['tables'].items():
            self.schema_elements.append({
                'type': 'table',
                'name': table_name,
                'full_name': table_name,
                'description': f"Table: {table_name}"
            })

            for col in table_info['columns']:
                self.schema_elements.append({
                    'type': 'column',
                    'name': col['name'],
                    'table': table_name,
                    'full_name': f"{table_name}.{col['name']}",
                    'description': f"Column: {col['name']} in {table_name} ({col['type']})"
                })

        descriptions = [elem['description'] for elem in self.schema_elements]
        self.embeddings = self.embedding_model.encode(descriptions)

        dimension = self.embeddings.shape[1]
        self.index = faiss.IndexFlatL2(dimension)
        self.index.add(self.embeddings.astype('float32'))

        print(f" Schema index built with {len(self.schema_elements)} elements")

    def link_terms(self, question: str, top_k: int = 10) -> List[Dict]:
        """Find relevant schema elements for the question"""
        question_embedding = self.embedding_model.encode([question])

        distances, indices = self.index.search(
            question_embedding.astype('float32'),
            min(top_k, len(self.schema_elements))
        )

        results = []
        for dist, idx in zip(distances[0], indices[0]):
            element = self.schema_elements[idx].copy()
            element['similarity'] = float(1 / (1 + dist))
            results.append(element)

        return results

    def get_relevant_tables(self, question: str) -> List[str]:
        """Get list of relevant table names"""
        linked = self.link_terms(question, top_k=5)
        tables = set()

        for elem in linked:
            if elem['type'] == 'table':
                tables.add(elem['name'])
            elif elem['type'] == 'column':
                tables.add(elem['table'])

        return list(tables)

linker = SchemaLinker(visualizer.schema_info)
test_question = "Show me customers who bought electronics"
linked_elements = linker.link_terms(test_question)

print(f"Schema linking for: '{test_question}'\n")
for elem in linked_elements[:5]:
    print(f"   {elem['full_name']} ({elem['type']}) - Similarity: {elem['similarity']:.3f}")

class SemanticCache:
    """Cache for storing and retrieving similar queries using embeddings"""
    def __init__(self, max_size: int = Config.MAX_CACHE_SIZE):
        self.max_size = max_size
        self.embedding_model = SentenceTransformer(Config.EMBEDDING_MODEL)
        self.cache_entries = []
        self.index = None
    def _rebuild_index(self):
        """Rebuild FAISS index"""
        if not self.cache_entries:
            return
        embeddings = np.array([entry['embedding'] for entry in self.cache_entries])
        dimension = embeddings.shape[1]
        self.index = faiss.IndexFlatL2(dimension)
        self.index.add(embeddings.astype('float32'))

    def add(self, question: str, sql: str, result: Any, metadata: Dict = None):
        """Add query to cache"""
        embedding = self.embedding_model.encode([question])[0]

        entry = {
            'question': question,
            'sql': sql,
            'result': result,
            'embedding': embedding,
            'metadata': metadata or {},
            'timestamp': datetime.now().isoformat(),
            'hit_count': 0
        }

        self.cache_entries.append(entry)

        if len(self.cache_entries) > self.max_size:
            self.cache_entries.sort(key=lambda x: x['hit_count'])
            self.cache_entries = self.cache_entries[-self.max_size:]

        self._rebuild_index()

    def search(self, question: str, threshold: float = Config.CACHE_SIMILARITY_THRESHOLD) -> Optional[Dict]:
        """Search for similar cached queries"""
        if not self.cache_entries or self.index is None:
            return None

        question_embedding = self.embedding_model.encode([question])

        distances, indices = self.index.search(question_embedding.astype('float32'), 1)

        if len(distances[0]) == 0:
            return None

        distance = distances[0][0]
        similarity = 1 / (1 + distance)

        if similarity >= threshold:
            idx = indices[0][0]
            entry = self.cache_entries[idx]
            entry['hit_count'] += 1
            entry['cache_similarity'] = float(similarity)

            print(f"ðŸ’¾ Cache HIT! Similarity: {similarity:.3f}")
            return entry

        return None

    def get_stats(self) -> Dict:
        """Get cache statistics"""
        return {
            'total_entries': len(self.cache_entries),
            'total_hits': sum(e['hit_count'] for e in self.cache_entries),
            'max_size': self.max_size
        }

cache = SemanticCache()
print("Semantic cache initialized")

class DataPreparationModule:
    """Handles intelligent data cleaning based on query context"""

    def __init__(self, db_path: str):
        self.db_path = db_path

    def analyze_data_quality(self, table: str, columns: List[str] = None) -> Dict:
        """Analyze data quality issues"""
        conn = sqlite3.connect(self.db_path)

        issues = {
            'table': table,
            'columns': {},
            'suggestions': []
        }

        if columns is None:
            cursor = conn.cursor()
            cursor.execute(f"PRAGMA table_info({table})")
            columns = [row[1] for row in cursor.fetchall()]

        for column in columns:
            try:
                df = pd.read_sql_query(f"SELECT {column} FROM {table}", conn)

                col_issues = {
                    'null_count': df[column].isnull().sum(),
                    'null_percentage': (df[column].isnull().sum() / len(df)) * 100,
                    'unique_values': df[column].nunique(),
                    'data_type': str(df[column].dtype)
                }

                if df[column].dtype == 'object':
                    sample_values = df[column].dropna().head(10).tolist()
                    if self._looks_like_date(sample_values):
                        col_issues['potential_date_column'] = True
                        issues['suggestions'].append(
                            f"Column '{column}' appears to contain dates but stored as text"
                        )

                issues['columns'][column] = col_issues

            except Exception as e:
                issues['columns'][column] = {'error': str(e)}

        conn.close()
        return issues

    def _looks_like_date(self, values: List) -> bool:
        """Check if values look like dates"""
        date_patterns = [
            r'\d{4}-\d{2}-\d{2}',  # YYYY-MM-DD
            r'\d{2}/\d{2}/\d{4}',  # MM/DD/YYYY
            r'\d{2}-\d{2}-\d{4}',  # DD-MM-YYYY
        ]

        if not values:
            return False

        match_count = 0
        for value in values[:5]:
            if isinstance(value, str):
                for pattern in date_patterns:
                    if re.match(pattern, value.strip()):
                        match_count += 1
                        break

        return match_count >= 3

    def prepare_for_query(self, question: str, relevant_tables: List[str]) -> Dict[str, Any]:
        """Prepare data based on query needs"""
        preparation_log = {
            'actions': [],
            'warnings': []
        }
        for table in relevant_tables:
            issues = self.analyze_data_quality(table)

            if any(word in question.lower() for word in ['date', 'when', 'year', 'month', 'recent', 'last']):
                for col, col_info in issues['columns'].items():
                    if col_info.get('potential_date_column'):
                        preparation_log['warnings'].append(
                            f"Date column '{table}.{col}' is stored as text. Consider using date functions."
                        )
                        preparation_log['actions'].append({
                            'type': 'date_conversion',
                            'table': table,
                            'column': col,
                            'suggestion': f"Use date() or strftime() functions for {col}"
                        })

        return preparation_log

prep_module = DataPreparationModule(db_path)
prep_result = prep_module.prepare_for_query(
    "Show orders from last month",
    ['orders']
)
print("Data Preparation Analysis:")
print(json.dumps(prep_result, indent=2))

class QueryComplexityAnalyzer:
    """Analyzes query complexity and suggests decomposition strategies"""

    def __init__(self, schema_info: Dict[str, Any]):
        self.schema_info = schema_info

    def analyze(self, question: str, relevant_tables: List[str]) -> Dict[str, Any]:
        """Analyze query complexity"""

        complexity_score = 0
        factors = []

        num_tables = len(relevant_tables)
        if num_tables > Config.COMPLEXITY_THRESHOLD:
            complexity_score += num_tables
            factors.append(f"Multiple tables involved: {num_tables}")

        agg_keywords = ['average', 'sum', 'count', 'total', 'maximum', 'minimum', 'avg', 'max', 'min']
        num_aggregations = sum(1 for keyword in agg_keywords if keyword in question.lower())
        if num_aggregations > 0:
            complexity_score += num_aggregations * 2
            factors.append(f"Aggregation operations: {num_aggregations}")

        if any(word in question.lower() for word in ['each', 'every', 'per', 'by', 'group']):
            complexity_score += 2
            factors.append("Grouping required")

        filter_keywords = ['where', 'which', 'that', 'with', 'having', 'only', 'who']
        num_filters = sum(1 for keyword in filter_keywords if keyword in question.lower())
        if num_filters > 2:
            complexity_score += num_filters
            factors.append(f"Multiple conditions: {num_filters}")

        if any(word in question.lower() for word in ['top', 'bottom', 'highest', 'lowest', 'best', 'worst']):
            complexity_score += 1
            factors.append("Sorting/ranking required")

        if any(phrase in question.lower() for phrase in ['more than average', 'compared to', 'not in']):
            complexity_score += 3
            factors.append("Potential subquery needed")

        if complexity_score <= 3:
            level = "SIMPLE"
            strategy = "direct_translation"
        elif complexity_score <= 7:
            level = "MODERATE"
            strategy = "step_by_step"
        else:
            level = "COMPLEX"
            strategy = "decomposition"

        return {
            'complexity_score': complexity_score,
            'level': level,
            'strategy': strategy,
            'factors': factors,
            'num_tables': num_tables
        }

    def suggest_decomposition(self, question: str, analysis: Dict) -> List[str]:
        """Suggest how to break down complex queries"""
        if analysis['strategy'] != 'decomposition':
            return [question]

        steps = []

        if 'Aggregation' in str(analysis['factors']):
            steps.append("Step 1: Identify base data to aggregate")
            steps.append("Step 2: Apply aggregation functions")

        if analysis['num_tables'] > 2:
            steps.append("Step 3: Join relevant tables")

        if 'Sorting' in str(analysis['factors']):
            steps.append("Step 4: Sort and limit results")

        return steps if steps else [question]

complexity_analyzer = QueryComplexityAnalyzer(visualizer.schema_info)
test_questions = [
    "Show all customers",
    "What's the average order value per customer?",
    "Show me the top 5 customers who spent more than average on electronics in the last 3 months"
]

print("Query Complexity Analysis:\n")
for q in test_questions:
    tables = linker.get_relevant_tables(q)
    analysis = complexity_analyzer.analyze(q, tables)
    print(f"Q: {q}")
    print(f"   Level: {analysis['level']} (Score: {analysis['complexity_score']})")
    print(f"   Strategy: {analysis['strategy']}")
    print(f"   Factors: {', '.join(analysis['factors'])}\n")

@dataclass
class ConversationTurn:
    """Represents a single turn in the conversation"""
    question: str
    sql: str
    result: Any
    timestamp: str
    metadata: Dict = field(default_factory=dict)

class ConversationContextManager:
    """Manages conversation history and context"""

    def __init__(self, max_turns: int = Config.MAX_CONTEXT_TURNS):
        self.max_turns = max_turns
        self.history: List[ConversationTurn] = []
        self.current_entities = {}

    def add_turn(self, question: str, sql: str, result: Any, metadata: Dict = None):
        """Add a conversation turn"""
        turn = ConversationTurn(
            question=question,
            sql=sql,
            result=result,
            timestamp=datetime.now().isoformat(),
            metadata=metadata or {}
        )

        self.history.append(turn)

        if len(self.history) > self.max_turns:
            self.history = self.history[-self.max_turns:]

        self._extract_entities(sql)

    def _extract_entities(self, sql: str):
        """Extract and track entities from SQL"""
        tables = re.findall(r'FROM\s+(\w+)', sql, re.IGNORECASE)
        for table in tables:
            self.current_entities['last_table'] = table

    def get_context_summary(self) -> str:
        """Get context summary for LLM"""
        if not self.history:
            return "No previous context."

        summary = "CONVERSATION HISTORY:\n"
        for i, turn in enumerate(self.history[-3:], 1):
            summary += f"\nTurn {i}:\n"
            summary += f"  User: {turn.question}\n"
            summary += f"  SQL: {turn.sql}\n"

        if self.current_entities:
            summary += f"\nCurrent Context: {self.current_entities}\n"

        return summary

    def resolve_references(self, question: str) -> str:
        """Resolve pronouns and references using context"""
        if not self.history:
            return question

        resolved = question

        if any(pronoun in question.lower() for pronoun in ['it', 'them', 'those', 'that']):
            last_table = self.current_entities.get('last_table', '')
            if last_table:
                resolved = question.replace('them', last_table).replace('it', last_table)

        if 'same' in question.lower() or 'also' in question.lower():
            if self.history:
                last_question = self.history[-1].question
                resolved = f"{question} (referring to: {last_question})"

        return resolved

    def clear(self):
        """Clear conversation history"""
        self.history = []
        self.current_entities = {}

context_manager = ConversationContextManager()
print(" Conversation context manager initialized")

class SQLSelfCorrection:
    """Reviews and corrects generated SQL"""

    def __init__(self, db_path: str, schema_info: Dict[str, Any]):
        self.db_path = db_path
        self.schema_info = schema_info

    def validate_and_correct(self, sql: str, question: str) -> Tuple[str, List[str]]:
        """Validate SQL and attempt corrections"""
        issues = []
        corrected_sql = sql

        try:
            sqlparse.parse(sql)
        except Exception as e:
            issues.append(f"Syntax error: {str(e)}")

        tables_in_query = self._extract_tables(sql)
        for table in tables_in_query:
            if table not in self.schema_info['tables']:
                issues.append(f"Table '{table}' does not exist")
                similar = self._find_similar_table(table)
                if similar:
                    corrected_sql = corrected_sql.replace(table, similar)
                    issues.append(f"  â†’ Corrected to '{similar}'")

        columns_in_query = self._extract_columns(sql)
        for table, column in columns_in_query:
            if table in self.schema_info['tables']:
                valid_columns = [c['name'] for c in self.schema_info['tables'][table]['columns']]
                if column not in valid_columns and column != '*':
                    issues.append(f"Column '{column}' not in table '{table}'")
                    similar = self._find_similar_column(table, column)
                    if similar:
                        corrected_sql = corrected_sql.replace(f"{table}.{column}", f"{table}.{similar}")
                        issues.append(f"  â†’ Corrected to '{similar}'")

        execution_error = self._test_execution(corrected_sql)
        if execution_error:
            issues.append(f"Execution error: {execution_error}")

            if len(issues) > 0:
                corrected_sql = self._llm_correction(sql, question, issues)

        return corrected_sql, issues

    def _extract_tables(self, sql: str) -> List[str]:
        """Extract table names from SQL"""
        tables = re.findall(r'(?:FROM|JOIN)\s+(\w+)', sql, re.IGNORECASE)
        return list(set(tables))

    def _extract_columns(self, sql: str) -> List[Tuple[str, str]]:
        """Extract (table, column) pairs from SQL"""
        matches = re.findall(r'(\w+)\.(\w+)', sql)
        return matches

    def _find_similar_table(self, table_name: str) -> Optional[str]:
        """Find similar table name"""
        table_name_lower = table_name.lower()
        for existing_table in self.schema_info['tables'].keys():
            if table_name_lower in existing_table.lower() or existing_table.lower() in table_name_lower:
                return existing_table
        return None

    def _find_similar_column(self, table: str, column_name: str) -> Optional[str]:
        """Find similar column name"""
        if table not in self.schema_info['tables']:
            return None

        column_name_lower = column_name.lower()
        valid_columns = [c['name'] for c in self.schema_info['tables'][table]['columns']]

        for col in valid_columns:
            if column_name_lower in col.lower() or col.lower() in column_name_lower:
                return col
        return None

    def _test_execution(self, sql: str) -> Optional[str]:
        """Test SQL execution"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            cursor.execute(f"EXPLAIN QUERY PLAN {sql}")
            conn.close()
            return None
        except Exception as e:
            return str(e)

    def _llm_correction(self, sql: str, question: str, issues: List[str]) -> str:
        """Use LLM to correct SQL"""
        try:
            prompt = f"""The following SQL has issues. Please correct it.

Question: {question}

SQL with issues:
{sql}

Issues found:
{chr(10).join(f'- {issue}' for issue in issues)}

Available schema:
{self._get_schema_summary()}

Provide ONLY the corrected SQL, nothing else."""

            response = model.generate_content(prompt)
            corrected = response.text.strip()

            corrected = self._extract_sql_from_text(corrected)
            return corrected

        except Exception as e:
            print(f"LLM correction failed: {e}")
            return sql

    def _extract_sql_from_text(self, text: str) -> str:
        """Extract SQL from text response"""
        text = re.sub(r'```sql\s*', '', text)
        text = re.sub(r'```\s*', '', text)

        match = re.search(r'(SELECT\s+.+)', text, re.IGNORECASE | re.DOTALL)
        if match:
            sql = match.group(1)
            if ';' in sql:
                sql = sql.split(';')[0]
            return sql.strip()

        return text.strip()

    def _get_schema_summary(self) -> str:
        """Get concise schema summary"""
        summary = ""
        for table_name, table_info in self.schema_info['tables'].items():
            cols = ", ".join([c['name'] for c in table_info['columns']])
            summary += f"{table_name}({cols})\n"
        return summary

corrector = SQLSelfCorrection(db_path, visualizer.schema_info)

bad_sql = "SELECT * FROM customer WHERE nama = 'John'"
corrected, issues = corrector.validate_and_correct(bad_sql, "Show customer named John")

print("SQL Self-Correction Test:")
print(f"Original: {bad_sql}")
print(f"Issues: {issues}")
print(f"Corrected: {corrected}")

class NL2SQLEngine:
    """Main engine for natural language to SQL conversion"""
    def __init__(self, db_path: str):
        self.db_path = db_path

        self.visualizer = SchemaVisualizer(db_path)
        self.schema_info = self.visualizer.schema_info
        self.schema_linker = SchemaLinker(self.schema_info)
        self.cache = SemanticCache()
        self.context_manager = ConversationContextManager()
        self.complexity_analyzer = QueryComplexityAnalyzer(self.schema_info)
        self.data_prep = DataPreparationModule(db_path)
        self.corrector = SQLSelfCorrection(db_path, self.schema_info)

        print("NL2SQL Engine initialized with all components")

    def process_question(self, question: str, use_cache: bool = True) -> Dict[str, Any]:
        """Process a natural language question"""

        print(f"\n{'='*60}")
        print(f"Question: {question}")
        print(f"{'='*60}")

        result = {
            'original_question': question,
            'resolved_question': question,
            'sql': None,
            'result': None,
            'execution_time': 0,
            'cache_hit': False,
            'complexity_analysis': None,
            'issues': [],
            'metadata': {}
        }

        start_time = datetime.now()

        resolved_question = self.context_manager.resolve_references(question)
        result['resolved_question'] = resolved_question
        print(f"\nðŸ”„ Resolved: {resolved_question}")

        if use_cache:
            cached = self.cache.search(resolved_question)
            if cached:
                result['sql'] = cached['sql']
                result['result'] = cached['result']
                result['cache_hit'] = True
                result['execution_time'] = (datetime.now() - start_time).total_seconds()
                print(f"Using cached result")
                return result

        print(f"\nLinking schema...")
        relevant_tables = self.schema_linker.get_relevant_tables(resolved_question)
        linked_elements = self.schema_linker.link_terms(resolved_question, top_k=8)
        result['metadata']['relevant_tables'] = relevant_tables
        print(f"   Tables: {', '.join(relevant_tables)}")

        print(f"\n Analyzing complexity...")
        complexity = self.complexity_analyzer.analyze(resolved_question, relevant_tables)
        result['complexity_analysis'] = complexity
        print(f"   Level: {complexity['level']} | Strategy: {complexity['strategy']}")

        print(f"\n Preparing data...")
        prep_info = self.data_prep.prepare_for_query(resolved_question, relevant_tables)
        if prep_info['warnings']:
            print(f"   Warnings: {prep_info['warnings']}")

        print(f"\nGenerating SQL...")
        sql = self._generate_sql(
            resolved_question,
            linked_elements,
            complexity,
            prep_info
        )
        result['sql'] = sql
        print(f"   SQL: {sql}")

        print(f"\n Validating and correcting...")
        corrected_sql, issues = self.corrector.validate_and_correct(sql, resolved_question)

        if issues:
            print(f"   Issues found: {len(issues)}")
            for issue in issues:
                print(f"     - {issue}")
            result['issues'] = issues

            if corrected_sql != sql:
                print(f"   Corrected SQL: {corrected_sql}")
                sql = corrected_sql
                result['sql'] = sql

        print(f"\nExecuting query...")
        try:
            query_result = self._execute_query(sql)
            result['result'] = query_result
            print(f" Success! Rows: {len(query_result)}")

            self.cache.add(resolved_question, sql, query_result, result['metadata'])

            self.context_manager.add_turn(resolved_question, sql, query_result, result['metadata'])

        except Exception as e:
            result['result'] = None
            result['issues'].append(f"Execution failed: {str(e)}")
            print(f" Execution failed: {e}")

        result['execution_time'] = (datetime.now() - start_time).total_seconds()
        print(f"\nTotal time: {result['execution_time']:.2f}s")

        return result

    def _generate_sql(self, question: str, linked_elements: List[Dict],
                     complexity: Dict, prep_info: Dict) -> str:
        """Generate SQL using LLM"""

        prompt = self._build_sql_generation_prompt(
            question,
            linked_elements,
            complexity,
            prep_info
        )

        try:
            response = model.generate_content(prompt)
            sql = response.text.strip()

            sql = self._clean_sql_response(sql)

            return sql

        except Exception as e:
            print(f" SQL generation error: {e}")
            raise

    def _build_sql_generation_prompt(self, question: str, linked_elements: List[Dict],
                                    complexity: Dict, prep_info: Dict) -> str:
        """Build comprehensive prompt for SQL generation"""

        prompt = f"""You are an expert SQL generator. Convert the natural language question to SQL.

DATABASE SCHEMA:
{self.visualizer.get_schema_text()}

RELEVANT SCHEMA ELEMENTS:
"""
        for elem in linked_elements[:5]:
            prompt += f"- {elem['full_name']} ({elem['type']})\n"

        prompt += f"""
CONVERSATION CONTEXT:
{self.context_manager.get_context_summary()}

COMPLEXITY ANALYSIS:
- Level: {complexity['level']}
- Strategy: {complexity['strategy']}
- Factors: {', '.join(complexity['factors'])}
"""

        if prep_info['warnings']:
            prompt += f"""
DATA PREPARATION NOTES:
{chr(10).join(f'- {w}' for w in prep_info['warnings'])}
"""

        prompt += f"""
QUESTION: {question}

INSTRUCTIONS:
1. Generate syntactically correct SQLite SQL
2. Use proper JOINs based on foreign key relationships
3. Include appropriate WHERE clauses for filtering
4. Use aggregation functions (COUNT, SUM, AVG) when needed
5. Add ORDER BY and LIMIT for ranking/top queries
6. Handle date comparisons properly (use date() function for text dates)
7. Return ONLY the SQL query, no explanations

SQL:"""

        return prompt

    def _clean_sql_response(self, sql: str) -> str:
        """Clean SQL from LLM response"""
        sql = re.sub(r'```sql\s*', '', sql)
        sql = re.sub(r'```\s*', '', sql)

        lines = sql.split('\n')
        sql_lines = []
        for line in lines:
            line = line.strip()
            if line and not line.startswith('--') and not line.startswith('#'):
                sql_lines.append(line)

        sql = ' '.join(sql_lines)

        match = re.search(r'(SELECT\s+.+?)(?:;|\n\n|$)', sql, re.IGNORECASE | re.DOTALL)
        if match:
            sql = match.group(1)

        return sql.strip()

    def _execute_query(self, sql: str) -> List[Dict]:
        """Execute SQL and return results as list of dicts"""
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()

        cursor.execute(sql)
        rows = cursor.fetchall()

        result = [dict(row) for row in rows]

        conn.close()
        return result

    def display_results(self, result: Dict):
        """Display results in a formatted way"""
        print(f"\n{'='*60}")
        print("RESULTS")
        print(f"{'='*60}")

        if result['cache_hit']:
            print(" Source: CACHE")

        if result['complexity_analysis']:
            print(f"Complexity: {result['complexity_analysis']['level']}")

        print(f"\n SQL Query:")
        print(f"   {result['sql']}")

        if result['issues']:
            print(f"\nâš ï¸ Issues Detected:")
            for issue in result['issues']:
                print(f"   - {issue}")

        if result['result']:
            print(f"\nData ({len(result['result'])} rows):")
            df = pd.DataFrame(result['result'])
            print(df.to_string(index=False))
        else:
            print("\n No results or execution failed")

        print(f"\n Time: {result['execution_time']:.2f}s")

engine = NL2SQLEngine(db_path)

def run_demo():
    """Run interactive demo with example questions"""

    print("="*70)
    print(" ADVANCED NL2SQL SYSTEM - INTERACTIVE DEMO")
    print("="*70)

    demo_questions = [
        "Show all customers",
        "What are the top 3 customers by total spending?",
        "How many orders were placed in 2024?",
        "Show me products in the Electronics category",
        "What's the average order value?",
        "Which customers bought laptops?",
        "How many orders did they place?",
        "Show me their email addresses",
    ]

    print("\n Running demo questions...\n")

    for i, question in enumerate(demo_questions, 1):
        print(f"\n{'#'*70}")
        print(f"Demo Question {i}/{len(demo_questions)}")
        print(f"{'#'*70}")

        result = engine.process_question(question)
        engine.display_results(result)

        import time
        time.sleep(1)

    print(f"\n{'='*70}")
    print("CACHE STATISTICS")
    print(f"{'='*70}")
    stats = engine.cache.get_stats()
    print(f"Total cached entries: {stats['total_entries']}")
    print(f"Total cache hits: {stats['total_hits']}")
    print(f"Max cache size: {stats['max_size']}")

run_demo()

def interactive_mode():
    """Interactive mode for user queries"""

    print("\n" + "="*70)
    print("INTERACTIVE QUERY MODE")
    print("="*70)
    print("Type your questions in natural language!")
    print("Commands:")
    print("  'exit' or 'quit' - Exit interactive mode")
    print("  'clear' - Reset conversation context")
    print("  'schema' - View database schema graph")
    print("  'stats' - Show cache statistics")
    print("  'history' - Show conversation history")
    print("="*70 + "\n")

    while True:
        try:
            question = input("\n Your question: ").strip()

            if not question:
                continue

            # Handle commands
            if question.lower() in ['exit', 'quit']:
                print(" Goodbye!")
                break

            elif question.lower() == 'clear':
                engine.context_manager.clear()
                print(" Context cleared!")
                continue

            elif question.lower() == 'schema':
                engine.visualizer.visualize_schema()
                continue

            elif question.lower() == 'stats':
                stats = engine.cache.get_stats()
                print(f"\n Cache Statistics:")
                print(f"   Total Entries: {stats['total_entries']}")
                print(f"   Total Hits: {stats['total_hits']}")
                print(f"   Max Size: {stats['max_size']}")
                continue

            elif question.lower() == 'history':
                print(f"\nConversation History:")
                if not engine.context_manager.history:
                    print("   No history yet!")
                else:
                    for i, turn in enumerate(engine.context_manager.history, 1):
                        print(f"\n   Turn {i}:")
                        print(f"   Q: {turn.question}")
                        print(f"   SQL: {turn.sql}")
                continue

            result = engine.process_question(question)
            engine.display_results(result)

        except KeyboardInterrupt:
            print("\n\nInterrupted. Goodbye!")
            break

        except Exception as e:
            print(f"\nError: {e}")
            print(" Try rephrasing your question or type 'schema' to see available data")

print(" Interactive mode ready!")
print(" Run: interactive_mode() to start chatting with your database")

def run_advanced_examples():
    """Run advanced examples showcasing all features"""

    print("\n" + "="*70)
    print(" ADVANCED FEATURES DEMONSTRATION")
    print("="*70)

    examples = {
        "Simple Query": {
            "question": "List all products",
            "features": ["Basic SELECT", "Schema linking"]
        },

        "Aggregation": {
            "question": "What's the total revenue from all orders?",
            "features": ["Aggregation", "SUM function"]
        },

        "Complex Join": {
            "question": "Show customer names with their order count and total spent",
            "features": ["Multiple JOINs", "GROUP BY", "Multiple aggregations"]
        },

        "Filtering & Sorting": {
            "question": "Show top 5 most expensive products in Electronics category",
            "features": ["JOIN", "WHERE clause", "ORDER BY", "LIMIT"]
        },

        "Date Handling": {
            "question": "How many orders were placed in February 2024?",
            "features": ["Date functions", "Filtering", "COUNT"]
        },

        "Subquery": {
            "question": "Which products have never been ordered?",
            "features": ["Subquery", "NOT IN", "Complex logic"]
        },

        "Context Follow-up 1": {
            "question": "Show customers who spent more than $2000",
            "features": ["Filtering with comparison"]
        },

        "Context Follow-up 2": {
            "question": "What did they buy?",
            "features": ["Context resolution", "Pronoun handling", "Query reuse"]
        },
    }

    for title, example in examples.items():
        print(f"\n{'='*70}")
        print(f"{title}")
        print(f"   Features: {', '.join(example['features'])}")
        print(f"{'='*70}")

        result = engine.process_question(example['question'])
        engine.display_results(result)

        import time
        time.sleep(0.5)

    # Show final statistics
    print(f"\n{'='*70}")
    print("SESSION STATISTICS")
    print(f"{'='*70}")

    cache_stats = engine.cache.get_stats()
    print(f"\n Cache Performance:")
    print(f"   Entries: {cache_stats['total_entries']}")
    print(f"   Hits: {cache_stats['total_hits']}")
    print(f"   Hit Rate: {(cache_stats['total_hits'] / max(len(examples), 1)) * 100:.1f}%")

    print(f"\n Conversation:")
    print(f"   Turns: {len(engine.context_manager.history)}")
    print(f"   Context entities: {engine.context_manager.current_entities}")

print(" Advanced examples ready!")
print(" Run: run_advanced_examples() to see all features in action")

class QueryAnalytics:
    """Analytics and benchmarking for the NL2SQL system"""

    def __init__(self, engine: NL2SQLEngine):
        self.engine = engine
        self.query_log = []

    def benchmark_queries(self, questions: List[str], runs: int = 1) -> pd.DataFrame:
        """Benchmark multiple queries"""

        print(f"Running benchmark with {len(questions)} queries, {runs} run(s) each...")

        results = []

        for run in range(runs):
            print(f"\n Run {run + 1}/{runs}")

            for i, question in enumerate(questions, 1):
                print(f"   [{i}/{len(questions)}] {question[:50]}...")

                if run == 0:
                    result = self.engine.process_question(question, use_cache=False)
                else:
                    result = self.engine.process_question(question, use_cache=True)

                results.append({
                    'run': run + 1,
                    'question': question,
                    'complexity': result['complexity_analysis']['level'] if result['complexity_analysis'] else 'N/A',
                    'execution_time': result['execution_time'],
                    'cache_hit': result['cache_hit'],
                    'success': result['result'] is not None,
                    'num_issues': len(result['issues']),
                    'num_results': len(result['result']) if result['result'] else 0
                })

        df = pd.DataFrame(results)
        return df

    def analyze_performance(self, df: pd.DataFrame):
        """Analyze benchmark results"""

        print("\n" + "="*70)
        print("PERFORMANCE ANALYSIS")
        print("="*70)

        print("\n Timing Statistics:")
        print(df.groupby('complexity')['execution_time'].agg(['mean', 'min', 'max', 'std']).round(3))

        print("\nCache Performance:")
        cache_stats = df.groupby('cache_hit').size()
        print(f"   Cache Hits: {cache_stats.get(True, 0)}")
        print(f"   Cache Misses: {cache_stats.get(False, 0)}")
        if True in cache_stats.index:
            print(f"   Hit Rate: {(cache_stats.get(True, 0) / len(df)) * 100:.1f}%")

        print("\nSuccess Rate:")
        success_rate = (df['success'].sum() / len(df)) * 100
        print(f"   Successful queries: {df['success'].sum()}/{len(df)} ({success_rate:.1f}%)")

        print("\nIssues Detected:")
        print(f"   Total issues: {df['num_issues'].sum()}")
        print(f"   Queries with issues: {(df['num_issues'] > 0).sum()}")

        print("\n Results Distribution:")
        print(df.groupby('complexity')['num_results'].agg(['mean', 'min', 'max']).round(1))

        if len(df) > 0:
            self._plot_performance(df)

    def _plot_performance(self, df: pd.DataFrame):
        """Plot performance metrics"""

        fig, axes = plt.subplots(2, 2, figsize=(14, 10))

        df.groupby('complexity')['execution_time'].mean().plot(
            kind='bar', ax=axes[0, 0], color='skyblue'
        )
        axes[0, 0].set_title('Average Execution Time by Complexity')
        axes[0, 0].set_ylabel('Time (seconds)')
        axes[0, 0].set_xlabel('Complexity Level')

        cache_counts = df['cache_hit'].value_counts()
        cache_counts.plot(kind='pie', ax=axes[0, 1], autopct='%1.1f%%',
                         labels=['Cache Miss', 'Cache Hit'], colors=['#ff9999', '#90ee90'])
        axes[0, 1].set_title('Cache Performance')
        axes[0, 1].set_ylabel('')

        axes[1, 0].hist(df['execution_time'], bins=20, color='lightcoral', edgecolor='black')
        axes[1, 0].set_title('Execution Time Distribution')
        axes[1, 0].set_xlabel('Time (seconds)')
        axes[1, 0].set_ylabel('Frequency')

        success_by_complexity = df.groupby('complexity')['success'].mean() * 100
        success_by_complexity.plot(kind='bar', ax=axes[1, 1], color='lightgreen')
        axes[1, 1].set_title('Success Rate by Complexity')
        axes[1, 1].set_ylabel('Success Rate (%)')
        axes[1, 1].set_xlabel('Complexity Level')
        axes[1, 1].set_ylim([0, 105])

        plt.tight_layout()
        plt.show()

analytics = QueryAnalytics(engine)

print("Analytics module ready!")
print(" Use analytics.benchmark_queries(questions) to benchmark performance")

def comprehensive_test():
    """Comprehensive test of all system features"""

    print("\n" + "="*70)
    print(" COMPREHENSIVE SYSTEM TEST")
    print("="*70)

    test_questions = [
        # Basic queries
        "Show all customers",
        "List all products",

        # Aggregations
        "How many orders are there?",
        "What's the average product price?",

        # Joins
        "Show customer names with their total spending",
        "List products with their category names",

        # Complex queries
        "Which customers bought products in the Electronics category?",
        "Show top 3 products by total quantity sold",

        # Date queries
        "How many orders in 2024?",

        # Contextual queries (these use conversation history)
        "Show all orders",
        "How many of them are completed?",  # Refers to orders
    ]

    print(f"\nTesting {len(test_questions)} queries...\n")

    # Run benchmark
    df = analytics.benchmark_queries(test_questions, runs=2)

    # Analyze results
    analytics.analyze_performance(df)

    # Show some example results
    print("\n" + "="*70)
    print("SAMPLE QUERY RESULTS")
    print("="*70)

    for question in test_questions[:3]:
        result = engine.process_question(question)
        engine.display_results(result)

    return df

print("Test suite ready!")
print("Run: test_results = comprehensive_test() to test the entire system")

class SystemUtilities:
    """Utility functions for the NL2SQL system"""

    @staticmethod
    def export_results_to_csv(result: Dict, filename: str = "query_results.csv"):
        """Export query results to CSV"""
        if result['result']:
            df = pd.DataFrame(result['result'])
            df.to_csv(filename, index=False)
            print(f"Results exported to {filename}")
        else:
            print("No results to export")

    @staticmethod
    def export_sql_to_file(result: Dict, filename: str = "generated_query.sql"):
        """Export generated SQL to file"""
        with open(filename, 'w') as f:
            f.write(f"-- Question: {result['original_question']}\n")
            f.write(f"-- Generated: {datetime.now().isoformat()}\n")
            f.write(f"-- Complexity: {result['complexity_analysis']['level'] if result['complexity_analysis'] else 'N/A'}\n\n")
            f.write(result['sql'] + ";\n")
        print(f"SQL exported to {filename}")

    @staticmethod
    def save_conversation_history(context_manager: ConversationContextManager,
                                 filename: str = "conversation_history.json"):
        """Save conversation history"""
        history_data = []
        for turn in context_manager.history:
            history_data.append({
                'question': turn.question,
                'sql': turn.sql,
                'timestamp': turn.timestamp,
                'metadata': turn.metadata
            })

        with open(filename, 'w') as f:
            json.dump(history_data, f, indent=2)
        print(f" Conversation history saved to {filename}")

    @staticmethod
    def generate_system_report(engine: NL2SQLEngine) -> str:
        """Generate comprehensive system report"""

        report = f"""
{'='*70}
NL2SQL SYSTEM REPORT
Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
{'='*70}

DATABASE INFORMATION:
  Database: {engine.db_path}
  Tables: {len(engine.schema_info['tables'])}
  Relationships: {len(engine.schema_info['relationships'])}

SCHEMA:
"""
        for table_name, table_info in engine.schema_info['tables'].items():
            report += f"\n   {table_name}\n"
            report += f"     Columns: {len(table_info['columns'])}\n"
            report += f"     Foreign Keys: {len(table_info['foreign_keys'])}\n"

        cache_stats = engine.cache.get_stats()
        report += f"""
CACHE STATISTICS:
  Total Entries: {cache_stats['total_entries']}
  Total Hits: {cache_stats['total_hits']}
  Max Size: {cache_stats['max_size']}

CONVERSATION:
  Total Turns: {len(engine.context_manager.history)}
  Max Context Turns: {engine.context_manager.max_turns}

CONFIGURATION:
  Model: {Config.GEMINI_MODEL}
  Embedding Model: {Config.EMBEDDING_MODEL}
  Cache Threshold: {Config.CACHE_SIMILARITY_THRESHOLD}
  Complexity Threshold: {Config.COMPLEXITY_THRESHOLD}

{'='*70}
"""
        return report

# Utility instance
utils = SystemUtilities()

print("Utility functions ready!")
print("Available utilities:")
print("   - utils.export_results_to_csv(result, filename)")
print("   - utils.export_sql_to_file(result, filename)")
print("   - utils.save_conversation_history(engine.context_manager, filename)")
print("   - utils.generate_system_report(engine)")

def final_demo():
    """Complete demonstration of the system"""

    print("\n" + "="*80)
    print(" " * 20 + " NL2SQL SYSTEM - COMPLETE DEMO ")
    print("="*80)

    print("\nSTEP 1: Database Schema Visualization")
    print("-" * 80)
    engine.visualizer.visualize_schema()

    print("\n STEP 2: Natural Language Queries")
    print("-" * 80)

    example_queries = [
        "Show me the top 3 customers by spending",
        "How many products are in stock?",
        "What's the average order value for completed orders?",
    ]

    for query in example_queries:
        result = engine.process_question(query)
        engine.display_results(result)

    print("\n STEP 3: System Report")
    print("-" * 80)
    report = utils.generate_system_report(engine)
    print(report)

    print("\nSTEP 4: How to Use This System")
    print("-" * 80)
    print("""
1. BASIC QUERY:
   result = engine.process_question("Your question here")
   engine.display_results(result)

2. INTERACTIVE MODE:
   interactive_mode()

3. RUN ADVANCED EXAMPLES:
   run_advanced_examples()

4. BENCHMARK PERFORMANCE:
   test_results = comprehensive_test()

5. EXPORT RESULTS:
   utils.export_results_to_csv(result, "output.csv")
   utils.export_sql_to_file(result, "query.sql")

6. VIEW SCHEMA:
   engine.visualizer.visualize_schema()

7. CLEAR CONVERSATION CONTEXT:
   engine.context_manager.clear()

8. CHECK CACHE STATS:
   print(engine.cache.get_stats())
    """)

    print("\nSystem is ready for your queries! ")
    print("="*80)

# Run the final demo
final_demo()

print("="*80)
print(" QUICK START - Try these examples!")
print("="*80)

print("\nSimple Query:")
print("   result = engine.process_question('Show all customers')")
print("   engine.display_results(result)")

print("\n Complex Query (will be cached):")
print("   result = engine.process_question('Show top 5 products by revenue')")

print("\Conversational Follow-up:")
print("   result1 = engine.process_question('Show customers from USA')")
print("   result2 = engine.process_question('How many orders did they place?')")

print("\n4 Interactive Mode:")
print("   interactive_mode()  # Start chatting!")

print("\n Run Full Demo:")
print("   run_advanced_examples()  # See all features")

print("\n" + "="*80)
print(" PRO TIPS:")
print("="*80)
print("â€¢ The system remembers context - you can use pronouns like 'them', 'it'")
print("â€¢ Similar queries are cached - second run will be much faster!")
print("â€¢ Use 'schema' command in interactive mode to see the database structure")
print("â€¢ Complex queries are automatically decomposed for better results")
print("â€¢ SQL is auto-corrected if issues are detected")
print("="*80)

print("\n" + "="*80)
print("RUNNING LIVE EXAMPLES...")
print("="*80)

live_examples = [
    "Show me all customers",
    "What's the total revenue?",
    "Which customer spent the most?",
]

for example in live_examples:
    print(f"\n{'â”€'*80}")
    result = engine.process_question(example)
    engine.display_results(result)
    print("â”€"*80)

print("\n All examples completed!")
print("\You're all set! The system is ready for your queries.")
print("   Start with: interactive_mode()")

def print_system_documentation():
    """Print comprehensive system documentation"""

    doc = """
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                     NL2SQL SYSTEM - COMPLETE DOCUMENTATION                    â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ðŸ“š SYSTEM COMPONENTS:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. ðŸ—ƒï¸  SCHEMA VISUALIZER
   â€¢ Extracts complete database schema
   â€¢ Visualizes tables and relationships as a graph
   â€¢ Provides text representation for LLM

   Usage:
     visualizer = SchemaVisualizer(db_path)
     visualizer.visualize_schema()

2. ðŸ”— SCHEMA LINKER
   â€¢ Maps natural language terms to database elements
   â€¢ Uses semantic embeddings for intelligent matching
   â€¢ Identifies relevant tables and columns

   Usage:
     linker = SchemaLinker(schema_info)
     relevant = linker.get_relevant_tables(question)

3. ðŸ’¾ SEMANTIC CACHE
   â€¢ Caches query results using vector embeddings
   â€¢ Retrieves similar past queries
   â€¢ Configurable similarity threshold

   Usage:
     cache = SemanticCache()
     cached_result = cache.search(question)

4. ðŸ’¬ CONVERSATION CONTEXT
   â€¢ Maintains conversation history
   â€¢ Resolves pronouns and references
   â€¢ Tracks entities across turns

   Usage:
     context = ConversationContextManager()
     resolved = context.resolve_references(question)

5. ðŸ” COMPLEXITY ANALYZER
   â€¢ Analyzes query difficulty
   â€¢ Suggests decomposition strategies
   â€¢ Determines processing approach

   Usage:
     analyzer = QueryComplexityAnalyzer(schema_info)
     analysis = analyzer.analyze(question, tables)

6. ðŸ§¹ DATA PREPARATION
   â€¢ Analyzes data quality
   â€¢ Detects format issues
   â€¢ Suggests data transformations

   Usage:
     prep = DataPreparationModule(db_path)
     prep_info = prep.prepare_for_query(question, tables)

7. ðŸ”§ SELF-CORRECTION
   â€¢ Validates generated SQL
   â€¢ Detects schema mismatches
   â€¢ Auto-corrects common errors

   Usage:
     corrector = SQLSelfCorrection(db_path, schema_info)
     corrected_sql, issues = corrector.validate_and_correct(sql, question)

8. ðŸ¤– MAIN ENGINE
   â€¢ Orchestrates all components
   â€¢ Generates SQL using Gemini LLM
   â€¢ Executes queries and returns results

   Usage:
     engine = NL2SQLEngine(db_path)
     result = engine.process_question(question)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ðŸŽ¯ KEY FEATURES:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âœ… Conversational Context      - Remembers previous queries
âœ… Semantic Caching            - Fast retrieval of similar queries
âœ… Schema Linking              - Intelligent term mapping
âœ… Complexity Analysis         - Adaptive query strategies
âœ… Query Decomposition         - Breaks down complex queries
âœ… Auto-correction             - Fixes SQL errors automatically
âœ… Data Quality Checks         - Suggests data improvements
âœ… Schema Visualization        - Interactive graph display

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ðŸ“– USAGE PATTERNS:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

BASIC QUERY:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
result = engine.process_question("Show all customers")
engine.display_results(result)

INTERACTIVE SESSION:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
interactive_mode()
# Then type questions naturally:
# "Show me electronics products"
# "Which ones cost more than $100?"
# "How many have we sold?"

EXPORT RESULTS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
result = engine.process_question("Your question")
utils.export_results_to_csv(result, "output.csv")
utils.export_sql_to_file(result, "query.sql")

BENCHMARKING:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
questions = ["Query 1", "Query 2", "Query 3"]
df = analytics.benchmark_queries(questions, runs=3)
analytics.analyze_performance(df)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
âš™ï¸  CONFIGURATION:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Modify Config class to adjust:
  â€¢ GEMINI_API_KEY           - Your Gemini API key
  â€¢ GEMINI_MODEL             - LLM model to use
  â€¢ EMBEDDING_MODEL          - Sentence transformer model
  â€¢ CACHE_SIMILARITY_THRESHOLD - Cache matching threshold (0-1)
  â€¢ MAX_CACHE_SIZE           - Maximum cached queries
  â€¢ MAX_CONTEXT_TURNS        - Conversation history length
  â€¢ COMPLEXITY_THRESHOLD     - Tables count for complex queries

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ðŸ› TROUBLESHOOTING:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Issue: "API key not found"
Fix:   Set Config.GEMINI_API_KEY = "your_key_here"

Issue: SQL errors persist
Fix:   Check schema with: engine.visualizer.visualize_schema()
       Clear context with: engine.context_manager.clear()

Issue: Wrong results
Fix:   Rephrase question more specifically
       Use table/column names explicitly

Issue: Slow performance
Fix:   Queries are cached after first run
       Check cache stats: engine.cache.get_stats()

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ðŸ“ž QUICK REFERENCE:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

engine.process_question(q)              # Process a query
engine.display_results(result)          # Show results
interactive_mode()                      # Start chat mode
run_advanced_examples()                 # Run demos
comprehensive_test()                    # Run full test suite
engine.visualizer.visualize_schema()    # Show schema graph
engine.context_manager.clear()          # Clear history
engine.cache.get_stats()                # Cache statistics
utils.generate_system_report(engine)    # System report

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                           ðŸŽ‰ Happy Querying! ðŸŽ‰                               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""
    print(doc)

# Print documentation
print_system_documentation()

print("\n" + "="*80)
print("ðŸ“š Full documentation displayed above!")
print("ðŸ’¡ Save it with: help(NL2SQLEngine) for class documentation")
print("="*80)

print("="*80)
print("FINAL SYSTEM VALIDATION")
print("="*80)

print("\n Testing Schema Visualizer...")
assert len(engine.schema_info['tables']) > 0, "Schema loading failed"
print("  Schema loaded successfully")

print("\n Testing Schema Linker...")
test_link = engine.schema_linker.get_relevant_tables("customers")
assert len(test_link) > 0, "Schema linking failed"
print(f"   Schema linker working (found {len(test_link)} tables)")

print("\n Testing Cache...")
cache_stats = engine.cache.get_stats()
print(f"   Cache initialized (capacity: {cache_stats['max_size']})")

print("\n Testing Context Manager...")
assert len(engine.context_manager.history) >= 0, "Context manager failed"
print(f"   Context manager working (history: {len(engine.context_manager.history)} turns)")

print("\n Testing Query Processing...")
try:
    test_result = engine.process_question("SELECT 1 as test")
    print("   Query processing working")
except Exception as e:
    print(f"    Query processing warning: {e}")

print("\n" + "="*80)
print(" ALL SYSTEMS OPERATIONAL!")
print("="*80)
print("\n Ready to use! Try:")
print("   â€¢ interactive_mode()          - Chat with your database")
print("   â€¢ run_advanced_examples()     - See advanced features")
print("   â€¢ comprehensive_test()        - Run full test suite")
print("\n" + "="*80)

comprehensive_test()